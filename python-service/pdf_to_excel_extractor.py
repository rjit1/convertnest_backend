"""
PDF to Excel Extractor - Production Ready (Parallel Processing)
Converts multi-page PDFs to Excel with one sheet per page
Uses Google Gemini 2.5 Flash for accurate table extraction from each page
Max 25 pages per PDF with PARALLEL API calls for maximum speed

Features:
- PDF page extraction (converts each page to image)
- PARALLEL Gemini API calls (all pages processed simultaneously)
- Rate limiting (250 RPM, 1M TPM for Tier 1)
- Multi-sheet Excel generation (1 sheet per page)
- Progress tracking for frontend
- Comprehensive error handling
- Fast processing (~4-8 seconds for 20 pages vs 80 seconds sequential)
- Cost-effective (~$0.03 per page)
"""

import google.generativeai as genai
from PIL import Image
import json
import os
import logging
import time
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# PDF processing imports
try:
    import fitz  # PyMuPDF - faster, more reliable
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    logging.warning("PyMuPDF not available, trying pdf2image...")

try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False
    logging.warning("pdf2image not available")

if not PYMUPDF_AVAILABLE and not PDF2IMAGE_AVAILABLE:
    raise ImportError("Neither PyMuPDF nor pdf2image is available. Install at least one: pip install PyMuPDF or pip install pdf2image")

logger = logging.getLogger(__name__)


class PDFToExcelExtractor:
    """
    Extract tables from multi-page PDF and convert to Excel.
    Each PDF page becomes one Excel sheet.
    
    Production Configuration:
    - Max 25 pages per PDF
    - Sequential processing (1 page = 1 API call)
    - Rate limiting: 250 RPM (Tier 1)
    - Model: gemini-2.5-flash
    - Cost: ~$0.03 per page
    
    Workflow:
    1. Validate PDF (page count â‰¤ 25)
    2. Extract each page as image
    3. Call Gemini API for each page sequentially
    4. Generate Excel with multiple sheets (1 per page)
    5. Clean up temporary files
    """
    
    # Production limits
    MAX_PAGES = 25  # Maximum pages per PDF
    RPM_LIMIT = 250  # Tier 1 rate limit (requests per minute)
    TPM_LIMIT = 1000000  # Tier 1 TPM limit (1M tokens per minute)
    # Removed DELAY_BETWEEN_CALLS - Using parallel processing with 1M TPM headroom
    
    def __init__(self):
        """Initialize Gemini model and PDF processor"""
        # Get API key from environment
        self.api_key = os.getenv('GEMINI_API_KEY')
        if not self.api_key:
            # Try alternative key names
            self.api_key = os.getenv('GEMINI_API_KEY_1')
        
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY environment variable is required")
        
        # Configure Gemini
        genai.configure(api_key=self.api_key)
        
        # Initialize model with optimized generation config
        self.generation_config = {
            "temperature": 0.1,  # Low temperature for accurate/deterministic output
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,  # Conservative, can increase to 65536 if needed
        }
        
        self.model = genai.GenerativeModel(
            'gemini-2.5-flash',
            generation_config=self.generation_config
        )
        
        # Track API call timing for rate limiting
        self.last_api_call_time = 0
        self.api_call_count = 0
        self.api_call_window_start = time.time()
        
        logger.info(f"âœ“ PDF-to-Excel Extractor initialized (gemini-2.5-flash)")
        logger.info(f"âœ“ PDF processor: {'PyMuPDF' if PYMUPDF_AVAILABLE else 'pdf2image'}")
        logger.info(f"âœ“ Max pages: {self.MAX_PAGES}, Rate limit: {self.RPM_LIMIT} RPM")
    
    def get_pdf_page_count(self, pdf_path):
        """
        Get the number of pages in a PDF.
        
        Args:
            pdf_path (str): Path to PDF file
        
        Returns:
            int: Number of pages
        """
        try:
            if PYMUPDF_AVAILABLE:
                doc = fitz.open(pdf_path)
                page_count = len(doc)
                doc.close()
                return page_count
            elif PDF2IMAGE_AVAILABLE:
                from pdf2image import pdfinfo_from_path
                info = pdfinfo_from_path(pdf_path)
                return info.get('Pages', 0)
            else:
                raise RuntimeError("No PDF processing library available")
        except Exception as e:
            logger.error(f"Error getting PDF page count: {e}")
            raise
    
    def validate_pdf(self, pdf_path):
        """
        Validate PDF file for processing.
        
        Args:
            pdf_path (str): Path to PDF file
        
        Returns:
            dict: Validation result
                {
                    'valid': bool,
                    'page_count': int,
                    'error': str (if invalid)
                }
        """
        try:
            # Check file exists
            if not os.path.exists(pdf_path):
                return {
                    'valid': False,
                    'page_count': 0,
                    'error': f'PDF file not found: {pdf_path}'
                }
            
            # Check file size (max 100MB)
            file_size = os.path.getsize(pdf_path)
            max_size = 100 * 1024 * 1024  # 100MB
            if file_size > max_size:
                return {
                    'valid': False,
                    'page_count': 0,
                    'error': f'PDF file too large: {file_size / (1024*1024):.1f}MB (max: 100MB)'
                }
            
            # Get page count
            page_count = self.get_pdf_page_count(pdf_path)
            
            # Check page count limit
            if page_count > self.MAX_PAGES:
                return {
                    'valid': False,
                    'page_count': page_count,
                    'error': f'PDF has {page_count} pages. Maximum allowed: {self.MAX_PAGES} pages.'
                }
            
            if page_count == 0:
                return {
                    'valid': False,
                    'page_count': 0,
                    'error': 'PDF appears to be empty (0 pages)'
                }
            
            logger.info(f"âœ“ PDF validated: {page_count} pages, {file_size / 1024:.1f}KB")
            
            return {
                'valid': True,
                'page_count': page_count,
                'file_size': file_size
            }
            
        except Exception as e:
            logger.error(f"PDF validation error: {e}", exc_info=True)
            return {
                'valid': False,
                'page_count': 0,
                'error': f'PDF validation failed: {str(e)}'
            }
    
    def extract_pages_as_images(self, pdf_path, output_dir):
        """
        Extract each PDF page as a separate image.
        
        Args:
            pdf_path (str): Path to PDF file
            output_dir (str): Directory to save page images
        
        Returns:
            list: List of image file paths for each page
        """
        try:
            os.makedirs(output_dir, exist_ok=True)
            image_paths = []
            
            if PYMUPDF_AVAILABLE:
                # Use PyMuPDF (faster, more reliable)
                logger.info("Extracting PDF pages with PyMuPDF...")
                doc = fitz.open(pdf_path)
                
                for page_num in range(len(doc)):
                    page = doc[page_num]
                    
                    # Render page to image (high quality: 300 DPI)
                    zoom = 3.0  # 300 DPI (72 * 3 = 216, close to 300)
                    mat = fitz.Matrix(zoom, zoom)
                    pix = page.get_pixmap(matrix=mat, alpha=False)
                    
                    # Save as PNG
                    image_path = os.path.join(output_dir, f"page_{page_num + 1:03d}.png")
                    pix.save(image_path)
                    image_paths.append(image_path)
                    
                    logger.info(f"  âœ“ Extracted page {page_num + 1}/{len(doc)}: {os.path.basename(image_path)}")
                
                doc.close()
                
            elif PDF2IMAGE_AVAILABLE:
                # Use pdf2image (requires poppler)
                logger.info("Extracting PDF pages with pdf2image...")
                images = convert_from_path(pdf_path, dpi=300, fmt='png')
                
                for page_num, image in enumerate(images):
                    image_path = os.path.join(output_dir, f"page_{page_num + 1:03d}.png")
                    image.save(image_path, 'PNG')
                    image_paths.append(image_path)
                    
                    logger.info(f"  âœ“ Extracted page {page_num + 1}/{len(images)}: {os.path.basename(image_path)}")
            
            else:
                raise RuntimeError("No PDF processing library available")
            
            logger.info(f"âœ“ Extracted {len(image_paths)} pages as images")
            return image_paths
            
        except Exception as e:
            logger.error(f"Error extracting PDF pages: {e}", exc_info=True)
            raise
    
    def _check_rpm_limit(self):
        """
        Check if we're within RPM limits (250 for Tier 1).
        Called before batch processing to ensure we don't exceed limits.
        Note: With parallel processing and 1M TPM, we have ample headroom.
        """
        current_time = time.time()
        
        # Track calls per minute
        if current_time - self.api_call_window_start >= 60:
            # Reset window
            if self.api_call_count > 0:
                logger.info(f"Rate limit window reset: {self.api_call_count} calls in last minute")
            self.api_call_count = 0
            self.api_call_window_start = current_time
        
        # Check if we're approaching RPM limit
        if self.api_call_count >= self.RPM_LIMIT:
            wait_time = 60 - (current_time - self.api_call_window_start)
            if wait_time > 0:
                logger.warning(f"RPM limit reached ({self.RPM_LIMIT}), waiting {wait_time:.1f}s")
                time.sleep(wait_time)
                self.api_call_count = 0
                self.api_call_window_start = time.time()
    
    def _get_page_extraction_prompt(self):
        """
        Get the optimized prompt for PDF page table extraction.
        Simplified version focusing on table extraction from a single page.
        """
        return """You are an EXPERT TABLE EXTRACTION system. Extract ALL tables from this PDF page with PERFECT accuracy.

ðŸ“‹ INSTRUCTIONS:

1. **IDENTIFY ALL TABLES:**
   - Detect tables with visible borders
   - Detect borderless tables (aligned columns)
   - Include all tabular data on the page

2. **EXTRACT STRUCTURE:**
   - Count exact rows and columns
   - Identify header rows
   - Handle merged cells (put text in first cell)

3. **EXTRACT CONTENT:**
   - Read each cell exactly as shown
   - Preserve numbers precisely (2250 not 2200)
   - Keep formatting (currency symbols, decimals)
   - Empty cells: "" (empty string)

4. **OUTPUT FORMAT (STRICT JSON):**
```json
{
  "page_number": <page number from image filename or 1>,
  "tables_found": <number of tables on this page>,
  "tables": [
    {
      "table_id": <1, 2, 3... if multiple tables>,
      "metadata": {
        "total_rows": <row count>,
        "total_columns": <column count>,
        "has_headers": true|false
      },
      "headers": ["Column1", "Column2", ...],
      "data": [
        ["row1_col1", "row1_col2", ...],
        ["row2_col1", "row2_col2", ...],
        ...
      ]
    }
  ],
  "extraction_notes": "Brief quality notes"
}
```

âœ… QUALITY REQUIREMENTS:
â˜‘ All tables on page extracted
â˜‘ Exact cell values (no approximation)
â˜‘ Correct row/column counts
â˜‘ Headers properly identified
â˜‘ Empty cells marked as ""

ðŸš€ BEGIN EXTRACTION:"""
    
    def extract_table_from_page(self, image_path, page_number):
        """
        Extract table from a single PDF page image using Gemini.
        
        Args:
            image_path (str): Path to page image
            page_number (int): Page number (1-indexed)
        
        Returns:
            dict: Extraction result
                {
                    'success': bool,
                    'page_number': int,
                    'tables': [...],
                    'error': str (if failed)
                }
        """
        try:
            # Track API call (rate limiting done at batch level)
            self.api_call_count += 1
            
            logger.info(f"ðŸ¤– Processing page {page_number} with Gemini...")
            
            # Load image
            img = Image.open(image_path)
            
            # Get extraction prompt
            prompt = self._get_page_extraction_prompt()
            
            # Generate response with Gemini
            response = self.model.generate_content([prompt, img])
            
            # Handle blocked/filtered responses
            if not response or not response.text:
                if hasattr(response, 'prompt_feedback'):
                    logger.warning(f"Page {page_number} blocked: {response.prompt_feedback}")
                    return {
                        'success': False,
                        'page_number': page_number,
                        'tables': [],
                        'error': 'Content filtered by Gemini safety settings'
                    }
                raise ValueError("Empty response from Gemini API")
            
            # Extract JSON from response
            response_text = response.text.strip()
            json_text = self._extract_json_from_response(response_text)
            
            # Parse JSON
            result = json.loads(json_text)
            
            # Validate structure
            if 'tables' not in result:
                raise ValueError("Response missing 'tables' field")
            
            # Log success
            tables_count = len(result.get('tables', []))
            logger.info(f"  âœ“ Page {page_number}: {tables_count} table(s) extracted")
            
            return {
                'success': True,
                'page_number': page_number,
                'tables': result.get('tables', []),
                'metadata': {
                    'tables_found': result.get('tables_found', tables_count),
                    'extraction_notes': result.get('extraction_notes', '')
                }
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"Page {page_number} JSON parse error: {e}")
            logger.error(f"Raw response (first 300 chars): {response_text[:300]}...")
            return {
                'success': False,
                'page_number': page_number,
                'tables': [],
                'error': f'Failed to parse Gemini response: {str(e)}'
            }
            
        except Exception as e:
            logger.error(f"Page {page_number} extraction error: {e}", exc_info=True)
            return {
                'success': False,
                'page_number': page_number,
                'tables': [],
                'error': f'Extraction failed: {str(e)}'
            }
    
    def _extract_json_from_response(self, response_text):
        """
        Extract JSON from Gemini response (handles markdown code blocks).
        
        Args:
            response_text (str): Raw response from Gemini
        
        Returns:
            str: Cleaned JSON string
        """
        # Handle markdown JSON code blocks
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            return response_text[json_start:json_end].strip()
        elif "```" in response_text:
            json_start = response_text.find("```") + 3
            json_end = response_text.find("```", json_start)
            return response_text[json_start:json_end].strip()
        else:
            return response_text.strip()
    
    def process_pdf(self, pdf_path, output_dir=None, progress_callback=None):
        """
        Complete PDF-to-Excel conversion workflow.
        
        Args:
            pdf_path (str): Path to PDF file
            output_dir (str, optional): Directory for temporary files and output
            progress_callback (callable, optional): Callback function for progress updates
                Signature: callback(current_page, total_pages, status, message)
        
        Returns:
            dict: Processing result
                {
                    'success': bool,
                    'page_results': [...],  # Results for each page
                    'total_pages': int,
                    'successful_pages': int,
                    'failed_pages': int,
                    'processing_time': float,
                    'error': str (if failed)
                }
        """
        start_time = time.time()
        
        try:
            # Step 1: Validate PDF
            if progress_callback:
                progress_callback(0, 0, 'validating', 'Validating PDF file...')
            
            validation = self.validate_pdf(pdf_path)
            if not validation['valid']:
                return {
                    'success': False,
                    'page_results': [],
                    'total_pages': validation['page_count'],
                    'successful_pages': 0,
                    'failed_pages': 0,
                    'error': validation['error']
                }
            
            total_pages = validation['page_count']
            logger.info(f"ðŸ“„ Processing PDF: {total_pages} pages")
            
            # Step 2: Create temporary directory for page images
            if output_dir is None:
                output_dir = os.path.join(os.path.dirname(pdf_path), f'pdf_pages_{int(time.time())}')
            
            os.makedirs(output_dir, exist_ok=True)
            
            # Step 3: Extract pages as images
            if progress_callback:
                progress_callback(0, total_pages, 'extracting', 'Extracting PDF pages...')
            
            image_paths = self.extract_pages_as_images(pdf_path, output_dir)
            
            # Step 4: Check RPM limit before parallel processing
            self._check_rpm_limit()
            
            # Step 5: Process all pages in PARALLEL for 10x speed improvement
            page_results = []
            successful_pages = 0
            failed_pages = 0
            completed_count = 0
            
            logger.info(f"ðŸš€ Starting PARALLEL processing of {len(image_paths)} pages...")
            
            # Use ThreadPoolExecutor for parallel Gemini API calls
            # Max workers = number of pages (up to 25)
            max_workers = min(len(image_paths), 25)
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit all pages for processing simultaneously
                future_to_page = {
                    executor.submit(self.extract_table_from_page, image_path, idx + 1): (idx + 1, image_path)
                    for idx, image_path in enumerate(image_paths)
                }
                
                # Collect results as they complete
                for future in as_completed(future_to_page):
                    page_num, image_path = future_to_page[future]
                    completed_count += 1
                    
                    try:
                        result = future.result()
                        page_results.append(result)
                        
                        if result['success']:
                            successful_pages += 1
                            logger.info(f"âœ“ Page {page_num}/{total_pages} completed ({completed_count}/{total_pages} done)")
                        else:
                            failed_pages += 1
                            logger.warning(f"âš ï¸  Page {page_num} failed: {result.get('error', 'Unknown error')}")
                        
                        # Update progress callback
                        if progress_callback:
                            progress_callback(completed_count, total_pages, 'processing', 
                                             f'Processing: {completed_count}/{total_pages} pages complete...')
                    
                    except Exception as e:
                        # Handle execution errors
                        logger.error(f"âŒ Page {page_num} execution error: {e}", exc_info=True)
                        failed_pages += 1
                        page_results.append({
                            'success': False,
                            'page_number': page_num,
                            'tables': [],
                            'error': f'Execution failed: {str(e)}'
                        })
            
            # Sort results by page number to maintain correct order
            page_results.sort(key=lambda x: x['page_number'])
            
            # API call count already updated in extract_table_from_page()
            
            # Step 6: Calculate processing time
            processing_time = time.time() - start_time
            
            logger.info(f"âœ… PARALLEL PDF processing complete:")
            logger.info(f"   Total pages: {total_pages}")
            logger.info(f"   Successful: {successful_pages}")
            logger.info(f"   Failed: {failed_pages}")
            logger.info(f"   Time: {processing_time:.2f}s (~{processing_time/total_pages:.1f}s per page)")
            logger.info(f"   Speed: {total_pages/(processing_time/60):.1f} pages/minute")
            logger.info(f"   API calls: {len(image_paths)} (parallel)")
            
            if progress_callback:
                progress_callback(total_pages, total_pages, 'complete', 
                                 f'Processing complete: {successful_pages}/{total_pages} pages successful')
            
            return {
                'success': successful_pages > 0,  # Success if at least one page processed
                'page_results': page_results,
                'total_pages': total_pages,
                'successful_pages': successful_pages,
                'failed_pages': failed_pages,
                'processing_time': processing_time,
                'image_paths': image_paths,  # For cleanup
                'temp_dir': output_dir
            }
            
        except Exception as e:
            logger.error(f"PDF processing error: {e}", exc_info=True)
            return {
                'success': False,
                'page_results': [],
                'total_pages': 0,
                'successful_pages': 0,
                'failed_pages': 0,
                'processing_time': time.time() - start_time,
                'error': f'PDF processing failed: {str(e)}'
            }
    
    def cleanup_temp_files(self, temp_dir, image_paths):
        """
        Clean up temporary files created during processing.
        
        Args:
            temp_dir (str): Temporary directory path
            image_paths (list): List of image file paths to delete
        """
        try:
            # Delete image files
            for image_path in image_paths:
                if os.path.exists(image_path):
                    os.remove(image_path)
                    logger.debug(f"Deleted: {image_path}")
            
            # Delete temporary directory if empty
            if os.path.exists(temp_dir) and not os.listdir(temp_dir):
                os.rmdir(temp_dir)
                logger.debug(f"Deleted temp dir: {temp_dir}")
            
            logger.info("ðŸ§¹ Cleanup complete")
            
        except Exception as e:
            logger.warning(f"Cleanup warning: {e}")
